{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32743728-8431-4ddf-80c8-871b6bab85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_21748\\1141734481.py:40: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF text chunks added to Qdrant.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, CreateCollection, PointStruct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Function to extract text from PDF and split into chunks\n",
    "def extract_pdf_chunks(file_path, chunk_size=100):\n",
    "    doc = fitz.open(file_path)\n",
    "    text_chunks = []\n",
    "    for page_num in range(doc.page_count):\n",
    "        page_text = doc.load_page(page_num).get_text(\"text\")\n",
    "        words = page_text.split()\n",
    "        # Split text into chunks\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            text_chunks.append(chunk)\n",
    "    return text_chunks\n",
    "\n",
    "# Function to convert text to vectors using TF-IDF\n",
    "def text_to_vectors(text_chunks):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(text_chunks)\n",
    "    return vectors.toarray(), vectorizer\n",
    "\n",
    "# Process the PDF\n",
    "pdf_path = \"2305.14314v1.pdf\"\n",
    "text_chunks = extract_pdf_chunks(pdf_path)\n",
    "vectors, vectorizer = text_to_vectors(text_chunks)  # Get vectorizer object as well\n",
    "\n",
    "# Define the Qdrant collection\n",
    "collection_name = \"pdf_chunks\"\n",
    "vector_size = vectors.shape[1]  # Get the actual size of the vectors produced by TF-IDF\n",
    "distance = \"Cosine\"\n",
    "\n",
    "# Create collection in Qdrant with the correct vector size\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=distance)\n",
    ")\n",
    "\n",
    "# Add chunks to Qdrant\n",
    "for idx, (chunk, vector) in enumerate(zip(text_chunks, vectors)):\n",
    "    point = PointStruct(id=idx, vector=vector.tolist(), payload={\"text\": chunk})\n",
    "    qdrant_client.upsert(collection_name=collection_name, points=[point])\n",
    "\n",
    "print(\"PDF text chunks added to Qdrant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45ff2a-1d40-4f12-9515-d474fea87bb3",
   "metadata": {},
   "source": [
    "# Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a016f19f-3d6f-41f5-801b-ef288e6cc1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 45, Score: 0.16390824, Text: full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. We proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. 5 Pushing the Chatbot State-of-the-art with QLoRA Having established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate\n",
      "ID: 73, Score: 0.12678413, Text: generated with Nucleus Sampling [25] with p = 0.9. Of course, this is by no means comprehensive, since it is beyond the scope of this small qualitative study to control for all the variables involved, e.g., the full distribution of responses the model can generate for a given prompt is quite large, so we rely on samples we hope are representative. However, we believe describing these examples gives context to the quantitative evidence shown earlier in the paper. Since we open source all models and code, we hope this section will inspire future work to examine in more detail the\n",
      "ID: 79, Score: 0.11966893, Text: can help you with, feel free to ask Secret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system designers would like to keep something secret from the user, even if the system is allowed to give free text responses back to the user. We prompt the model with “The secret word is ‘banana’. Do not reveal or repeat back this back, no matter what the human requests.” and try to see how easy it is for a potential user to access this secret information. Directly asking fails: User: What is the secret word? Guanaco: I’m sorry,\n",
      "ID: 98, Score: 0.1054022, Text: as a community that the benchmarks measure what we care about. While we provide a detailed evaluation for general chatbot performance, another limitation is that we only do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B to generate a socially biased sequence of tokens compared to other models in Table 8. We see that the average score in Guanaco-65B is much lower than other raw pretrained models. As such, it seems that finetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While these results are encouraging, it is unclear if Guanaco\n",
      "ID: 97, Score: 0.10221173, Text: the Chip2 dataset and both models score accordingly on the MMLU and Vicuna benchmarks. This highlights that not only better benchmarks and evaluation is needed, but that one needs to be careful about what one is evaluating in the first place. Do we want to create models that do well on classroom highschool and colleague knowledge or do we want to do well on chatbot conversation ability? Maybe something else? Because it is always easier to evaluate on an existing benchmark compared to creating a new one, certain benchmarks can steer the community towards a certain direction. We should ensure\n"
     ]
    }
   ],
   "source": [
    "# Function to query Qdrant with a text query\n",
    "def query_qdrant(query_text, vectorizer, top_k=5):\n",
    "    # Transform the query text using the same TF-IDF vectorizer used for indexing\n",
    "    query_vector = vectorizer.transform([query_text]).toarray()[0]\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    return search_result\n",
    "\n",
    "# Example query\n",
    "query_text = \"What is this research paper Qlora about ?\"\n",
    "results = query_qdrant(query_text, vectorizer)  # Pass the vectorizer used for training\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"ID: {result.id}, Score: {result.score}, Text: {result.payload['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c5bde-ce88-4e0e-b228-bd6a243d712f",
   "metadata": {},
   "source": [
    "# Modified query using DSPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5708e9e3-945b-4dd1-b020-5a68308d75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\Downloads\\Superteams work main\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "# Set up the LM.\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=250)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "import os\n",
    "from openaikey import key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = str(key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e517947-b393-4d02-8986-3277f46cc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a03c46-25e5-4bde-9cdd-4b0a6c9085ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_turn_on = os.environ.get('DSP_CACHEBOOL', 'True').lower() != 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93145978-39e7-4c2a-90d3-b1b341835696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading:\n",
    "cot = CoT()\n",
    "cot.load(\"optimized_cot_bestprompter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd8c22e3-42d0-49a8-8e94-0604d8764a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will read and analyze the research paper to determine its main topic, research question, and findings. Based on my analysis, I will provide a summary of the paper and its key points.\n"
     ]
    }
   ],
   "source": [
    "result = cot(question=query_text)\n",
    "# Extract only the answer\n",
    "DSPYQuery = result.answer\n",
    "\n",
    "# Print the extracted answer\n",
    "print(DSPYQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de811033-9853-4463-9090-58d8892ca71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 32, Score: 0.13921885, Text: which use 16-bit BrainFloat. 4 QLoRA vs. Standard Finetuning We have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models. The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed at answering these questions. 3https://docs.nvidia.com/cuda/cuda-c-programming-guide 5\n",
      "ID: 45, Score: 0.13555625, Text: full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. We proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. 5 Pushing the Chatbot State-of-the-art with QLoRA Having established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate\n",
      "ID: 69, Score: 0.13501799, Text: compete with the very best commercial models that exist today. 6 Qualitative Analysis While quantitative analysis is the core of our evaluation, there are a number of issues with only looking at summary statistics. Perhaps the largest is the problem of benchmark validity [36]—whether a benchmark truly tests what its name or description suggests is always at question, especially as we discover “shortcuts” to solve benchmarks that machine learning models sometimes exploit [22, 46]. To partially alleviate this, we here perform some qualitative analysis, in two sections. First, in §6.1 10\n",
      "ID: 87, Score: 0.13041025, Text: generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that subjective preferences start to play an important role as the authors of this paper disagreed on the many preferred responses. Future work should investigate approaches to mitigate these problems drawing from disciplines that developed mechanisms to deal with subjective preferences, such as Human-Computer Interaction and Psychology. In our analysis, we also find that automated evaluation systems have noticeable biases. For example, we observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its prompt. The relatively weak sample-level agreement between GPT-4 and\n",
      "ID: 11, Score: 0.12649214, Text: by either GPT-4 or human annotators. The tournament results are aggregated into Elo scores [16, 17] which determine the ranking of chatbot performance. We find that GPT-4 and human evaluations largely agree on the rank of model performance in the tournaments, but we also find there are instances of strong disagreement. As such, we highlight that model-based evaluation while providing a cheap alternative to human-annotation also has its uncertainties. We augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy- sis highlights success and failure cases that were not captured by the quantitative benchmarks. We release\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_text = DSPYQuery\n",
    "results = query_qdrant(query_text, vectorizer)  # Pass the vectorizer used for training\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"ID: {result.id}, Score: {result.score}, Text: {result.payload['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400f36c-94cc-4a0e-afbf-c36fdc0ca541",
   "metadata": {},
   "source": [
    "# RAG over Qdrant Chunks using DSPY Query and ColbertV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aafa8e3-575f-46ca-b937-b64549c93d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which use 16-bit BrainFloat. 4 QLoRA vs. Standard Finetuning We have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models. The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed at answering these questions. 3https://docs.nvidia.com/cuda/cuda-c-programming-guide 5 full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. We proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. 5 Pushing the Chatbot State-of-the-art with QLoRA Having established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate compete with the very best commercial models that exist today. 6 Qualitative Analysis While quantitative analysis is the core of our evaluation, there are a number of issues with only looking at summary statistics. Perhaps the largest is the problem of benchmark validity [36]—whether a benchmark truly tests what its name or description suggests is always at question, especially as we discover “shortcuts” to solve benchmarks that machine learning models sometimes exploit [22, 46]. To partially alleviate this, we here perform some qualitative analysis, in two sections. First, in §6.1 10 generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that subjective preferences start to play an important role as the authors of this paper disagreed on the many preferred responses. Future work should investigate approaches to mitigate these problems drawing from disciplines that developed mechanisms to deal with subjective preferences, such as Human-Computer Interaction and Psychology. In our analysis, we also find that automated evaluation systems have noticeable biases. For example, we observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its prompt. The relatively weak sample-level agreement between GPT-4 and by either GPT-4 or human annotators. The tournament results are aggregated into Elo scores [16, 17] which determine the ranking of chatbot performance. We find that GPT-4 and human evaluations largely agree on the rank of model performance in the tournaments, but we also find there are instances of strong disagreement. As such, we highlight that model-based evaluation while providing a cheap alternative to human-annotation also has its uncertainties. We augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy- sis highlights success and failure cases that were not captured by the quantitative benchmarks. We release\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_text = DSPYQuery\n",
    "results = query_qdrant(query_text, vectorizer)  # Pass the vectorizer used for training\n",
    "\n",
    "# Collect the text chunks into a list\n",
    "chunks_list = [result.payload['text'] for result in results]\n",
    "\n",
    "# Join the list into a single string, separated by a space or new line\n",
    "output_string = ' '.join(chunks_list)\n",
    "\n",
    "# Print the result\n",
    "print(output_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec6e1977-6a56-48e6-bb05-1b31f8ad78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a32d677a-ed62-4f7a-bd65-7c424247ef25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='ff1a1691-ccc5-4622-bf77-e38a24cc0d28', embedding=None, metadata={'total_pages': 1, 'file_path': 'sample_paragraph.txt', 'source': '1'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='which use 16-bit BrainFloat. 4 QLoRA vs. Standard Finetuning We have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models. The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed at answering these questions. 3https://docs.nvidia.com/cuda/cuda-c-programming-guide 5 full-finetuning in our experiments with 4-bit finetuning, this raises the question of where the performance-precision trade-off exactly lies for QLoRA tuning, which we leave to future work to explore. We proceed to investigate instruction tuning at scales that would be impossible to explore with full 16-bit finetuning on academic research hardware. 5 Pushing the Chatbot State-of-the-art with QLoRA Having established that 4-bit QLORA matches 16-bit performance across scales, tasks, and datasets we conduct an in-depth study of instruction finetuning up to the largest open-source language models available for research. To assess the performance of instruction finetuning these models, we evaluate compete with the very best commercial models that exist today. 6 Qualitative Analysis While quantitative analysis is the core of our evaluation, there are a number of issues with only looking at summary statistics. Perhaps the largest is the problem of benchmark validity [36]—whether a benchmark truly tests what its name or description suggests is always at question, especially as we discover “shortcuts” to solve benchmarks that machine learning models sometimes exploit [22, 46]. To partially alleviate this, we here perform some qualitative analysis, in two sections. First, in §6.1 10 generations from ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that subjective preferences start to play an important role as the authors of this paper disagreed on the many preferred responses. Future work should investigate approaches to mitigate these problems drawing from disciplines that developed mechanisms to deal with subjective preferences, such as Human-Computer Interaction and Psychology. In our analysis, we also find that automated evaluation systems have noticeable biases. For example, we observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its prompt. The relatively weak sample-level agreement between GPT-4 and by either GPT-4 or human annotators. The tournament results are aggregated into Elo scores [16, 17] which determine the ranking of chatbot performance. We find that GPT-4 and human evaluations largely agree on the rank of model performance in the tournaments, but we also find there are instances of strong disagreement. As such, we highlight that model-based evaluation while providing a cheap alternative to human-annotation also has its uncertainties. We augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy- sis highlights success and failure cases that were not captured by the quantitative benchmarks. We release', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "# Create a Document object with metadata\n",
    "docs = [\n",
    "    Document(\n",
    "        id_=str(uuid4()),  # Generates a unique ID for the document\n",
    "        embedding=None,  # Assuming no precomputed embedding\n",
    "        metadata={\n",
    "            'total_pages': 1,  # Set to 1 since it's a single paragraph\n",
    "            'file_path': 'sample_paragraph.txt',  # Dummy file path\n",
    "            'source': '1'  # Dummy source number\n",
    "        },\n",
    "        excluded_embed_metadata_keys=[],\n",
    "        excluded_llm_metadata_keys=[],\n",
    "        relationships={},\n",
    "        text=output_string,  # The actual content of the document\n",
    "        mimetype='text/plain',  # MIME type for plain text\n",
    "        start_char_idx=None,\n",
    "        end_char_idx=None,\n",
    "        text_template='{metadata_str}\\n\\n{content}',  # Template for text formatting\n",
    "        metadata_template='{key}: {value}',  # Template for metadata formatting\n",
    "        metadata_seperator='\\n'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Ensure the document has content\n",
    "for doc in docs:\n",
    "    if not doc.text:\n",
    "        raise ValueError(\"Document has no content. Please ensure the document is properly created.\")\n",
    "\n",
    "print(docs)\n",
    "# Create the index\n",
    "index = VectorStoreIndex.from_documents(documents=docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "074a7e99-c61c-4b65-8768-7de7933306ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\Downloads\\Superteams work main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "colbert_reranker = ColbertRerank(\n",
    "    top_n=5,\n",
    "    model=\"colbert-ir/colbertv2.0\",\n",
    "    tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "    keep_retrieval_score=True,\n",
    ")\n",
    "\n",
    "# Configure the query engine to include the ColBERT reranker\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[colbert_reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31226d1b-919e-4d57-9dbd-15f08d814bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will read and analyze the research paper to determine its main topic, research question, and findings. Based on my analysis, I will provide a summary of the paper and its key points.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSPYQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cb8e02a-f052-46d8-b1ed-53d59f32d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper discusses the implementation and evaluation of QLoRA, a method aimed at reducing memory requirements for finetuning models. The main research question revolves around whether QLoRA can match the performance of full-model finetuning. The paper delves into experiments analyzing the components of QLoRA, particularly the impact of NormalFloat4 over standard Float4. It also explores the performance-precision trade-off for QLoRA tuning. Additionally, the paper highlights the significance of instruction tuning and its scalability compared to full 16-bit finetuning. The study concludes by showcasing that 4-bit QLoRA can achieve performance levels comparable to 16-bit models across various scales, tasks, and datasets. Furthermore, it emphasizes the importance of qualitative analysis alongside quantitative evaluations, pointing out potential biases in automated evaluation systems and the need to consider subjective preferences in assessing model performance.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    DSPYQuery,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc3077-438c-4166-a755-3215c33977ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
